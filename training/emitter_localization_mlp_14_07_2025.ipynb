{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe40d620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pipe_mlp.py\n",
    "import numpy as np, torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ───────────────── CONFIG ──────────────────\n",
    "NPZ_PATH   = \"pipe_network_data.npz\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS     = 40\n",
    "LR         = 3e-4\n",
    "LAMBDA_Z   = 10.0          # weight for z-regression loss\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "N_SLOTS    = 1365\n",
    "F_IN       = 7\n",
    "# ────────────────────────────────────────────\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ad93d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Dataset wrapper -----------------\n",
    "class PipeDataset(Dataset):\n",
    "    def __init__(self, split):\n",
    "        d = np.load(NPZ_PATH)\n",
    "        self.X   = torch.tensor(d[f\"{split}_features\"],     dtype=torch.float32)\n",
    "        self.y_p = torch.tensor(d[f\"{split}_pipe_labels\"], dtype=torch.long)\n",
    "        self.y_z = torch.tensor(d[f\"{split}_z_labels\"],    dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    def __len__(self):              return len(self.y_p)\n",
    "    def __getitem__(self, idx):     return self.X[idx], self.y_p[idx], self.y_z[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f16dd2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- MLP with masking ---------------\n",
    "class MaskedMLP(nn.Module):\n",
    "    def __init__(self, in_slots=N_SLOTS, feat_dim=F_IN, hidden=2048, classes=N_SLOTS):\n",
    "        super().__init__()\n",
    "        flat_in = in_slots * feat_dim\n",
    "        self.mask_index = feat_dim - 1          # mask is last feature (index 6)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(flat_in, hidden),\n",
    "            nn.BatchNorm1d(hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden, hidden//2),\n",
    "            nn.BatchNorm1d(hidden//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.pipe_head = nn.Linear(hidden//2, classes)\n",
    "        self.z_head    = nn.Linear(hidden//2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [B, 1365, 7]\n",
    "        # zero-out features for missing slots using the mask bit\n",
    "        mask = x[:, :, self.mask_index: self.mask_index+1]   # [B, n, 1]\n",
    "        x_use = x.clone()\n",
    "        x_use[:, :, :-1] = x_use[:, :, :-1] * (1.0 - mask)   # keep mask channel itself\n",
    "        x_flat = x_use.view(x.size(0), -1)\n",
    "        h  = self.net(x_flat)\n",
    "        return self.pipe_head(h), self.z_head(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11f949d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Training / validation ----------\n",
    "def run_epoch(loader, model, opt=None):\n",
    "    train_mode = opt is not None\n",
    "    if train_mode:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    loss_pipe_sum = loss_z_sum = 0.0\n",
    "    correct = n = 0\n",
    "\n",
    "    for X, y_p, y_z in loader:\n",
    "        X, y_p, y_z = X.to(DEVICE), y_p.to(DEVICE), y_z.to(DEVICE)\n",
    "\n",
    "        if train_mode:\n",
    "            opt.zero_grad()\n",
    "\n",
    "        p_logits, z_pred = model(X)\n",
    "        loss_pipe = F.cross_entropy(p_logits, y_p)\n",
    "        loss_z    = F.mse_loss(z_pred, y_z)\n",
    "        loss      = loss_pipe + LAMBDA_Z * loss_z\n",
    "\n",
    "        if train_mode:\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        loss_pipe_sum += loss_pipe.item() * X.size(0)\n",
    "        loss_z_sum    += loss_z.item()    * X.size(0)\n",
    "        preds          = p_logits.argmax(1)\n",
    "        correct       += (preds == y_p).sum().item()\n",
    "        n             += X.size(0)\n",
    "\n",
    "    return (loss_pipe_sum/n, loss_z_sum/n, correct/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81438d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    train_ds = PipeDataset(\"train\")\n",
    "    val_ds   = PipeDataset(\"val\")\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE)\n",
    "\n",
    "    model = MaskedMLP().to(DEVICE)\n",
    "    opt   = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        tr_lp, tr_lz, tr_acc = run_epoch(train_loader, model, opt)\n",
    "        va_lp, va_lz, va_acc = run_epoch(val_loader,   model)\n",
    "\n",
    "        print(f\"Ep {ep:02d} | \"\n",
    "              f\"Train CE {tr_lp:.4f}  MSE {tr_lz:.4f}  Acc {tr_acc:.3f} | \"\n",
    "              f\"Val CE {va_lp:.4f}  MSE {va_lz:.4f}  Acc {va_acc:.3f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"pipe_mlp.pt\")\n",
    "    print(\"✔ saved pipe_mlp.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26301bf0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      2\u001b[39m train_ds = PipeDataset(\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m val_ds   = PipeDataset(\u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m train_loader = \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE)\n\u001b[32m      8\u001b[39m model = MaskedMLP().to(DEVICE)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/molecular-simulation-mlp/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:385\u001b[39m, in \u001b[36mDataLoader.__init__\u001b[39m\u001b[34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device, in_order)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m         sampler = \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    387\u001b[39m         sampler = SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/molecular-simulation-mlp/.venv/lib/python3.13/site-packages/torch/utils/data/sampler.py:156\u001b[39m, in \u001b[36mRandomSampler.__init__\u001b[39m\u001b[34m(self, data_source, replacement, num_samples, generator)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    152\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.replacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    153\u001b[39m     )\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_samples <= \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    157\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.num_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    158\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caba508c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
