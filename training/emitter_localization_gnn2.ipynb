{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "74f00316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_gnn.py\n",
    "import os, glob, json, math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.data import Data, Dataset, DataLoader\n",
    "from torch_geometric.nn import GraphSAGE, global_max_pool\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_geometric.utils import add_self_loops  # add this import\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "MAX_PIPES   = 20          # upper bound; graphs may be smaller\n",
    "R_STATS_DIM = 7           # how many per-receiver stats you saved\n",
    "DEVICE      = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e8c374a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------  small helpers ---------------------------------------------------\n",
    "def downsample(ts, T=256):\n",
    "    \"\"\"Uniformly down-sample / pad a 1-D list to length T.\"\"\"\n",
    "    if len(ts) >= T:\n",
    "        idx = np.linspace(0, len(ts)-1, T, dtype=int)\n",
    "        return np.array(ts)[idx]\n",
    "    out = np.zeros(T, dtype=float)\n",
    "    out[:len(ts)] = ts\n",
    "    return out\n",
    "\n",
    "def encode_receiver(stats):\n",
    "    \"\"\"\n",
    "    stats = list[float]  length R_STATS_DIM\n",
    "    Very small 1-layer MLP encoder.  Returns 16-D vector.\n",
    "    \"\"\"\n",
    "    w = np.array([0.2,0.2,0.2,0.1,0.1,0.1,0.1])[:R_STATS_DIM]\n",
    "    return (np.array(stats)*w).astype(np.float32)        # shape (R_STATS_DIM,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b4164aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------  Dataset class ---------------------------------------------------\n",
    "class VeinTreeDataset(Dataset):\n",
    "    def __init__(self, root):\n",
    "        super().__init__(root)\n",
    "        self.run_dirs = [d for d in os.listdir(root) if not d.startswith('.')]\n",
    "        self.run_dirs.sort()\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.run_dirs)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    def get(self, idx):\n",
    "        run_path = os.path.join(self.root, self.run_dirs[idx])\n",
    "        pipe_folders = sorted([p for p in os.listdir(run_path) if p.startswith('pipe')],\n",
    "                              key=lambda x: int(x.replace('pipe','')))\n",
    "        # first pass — collect structural info ---------------------------------\n",
    "        parent = {}\n",
    "        length = {}\n",
    "        radius = {}\n",
    "        recs   = {}           # pipe_id → list[(r,z,stats)]\n",
    "        for pf in pipe_folders:\n",
    "            pid = int(pf.replace('pipe',''))\n",
    "            with open(os.path.join(run_path, pf, 'simulation_data.txt')) as f:\n",
    "                t = f.readline().split()\n",
    "            pid = int(t[0])\n",
    "            try:                                    # robust parent-id parsing\n",
    "                par = int(t[1])\n",
    "            except ValueError:                      # e.g. 'None' or 'Non'\n",
    "                par = -1\n",
    "            L, R = map(float, t[2:4])\n",
    "            parent[pid]  = par\n",
    "            length[pid]  = L\n",
    "            radius[pid]  = R\n",
    "            files = glob.glob(os.path.join(run_path,pf,'#*-Ring type.txt'))\n",
    "            for rf in files:\n",
    "                with open(rf) as f:\n",
    "                    rline = f.readline().split()\n",
    "                    slist = [float(x.strip()) for x in f.readline().split(',')[:R_STATS_DIM]]\n",
    "                recs.setdefault(pid, []).append((float(rline[1]), float(rline[2]), slist))\n",
    "        # second pass — build PyG graph ---------------------------------------\n",
    "        pipe_ids = sorted(parent.keys())                # local idx 0…N-1\n",
    "        id2row   = {pid:i for i,pid in enumerate(pipe_ids)}\n",
    "        # build directed edges parent → child\n",
    "        edge_src, edge_dst = [], []\n",
    "        for pid in pipe_ids:\n",
    "            par = parent[pid]\n",
    "            if par > 0 and par in id2row:\n",
    "                edge_src.append(id2row[par])\n",
    "                edge_dst.append(id2row[pid])\n",
    "\n",
    "        edge_index = torch.tensor([edge_src + edge_dst,\n",
    "                                edge_dst + edge_src],          # add reverse\n",
    "                                dtype=torch.long)\n",
    "\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=len(pipe_ids))\n",
    "        # ---------- node features --------------------------------------------\n",
    "        N = len(pipe_ids)\n",
    "        x = []\n",
    "        for pid in pipe_ids:\n",
    "            has_rec = 1.0 if pid in recs else 0.0\n",
    "            # aggregate receiver stats (mean) or zeros\n",
    "            if pid in recs:\n",
    "                v = np.mean([encode_receiver(s) for _,_,s in recs[pid]], axis=0)\n",
    "            else:\n",
    "                v = np.zeros(R_STATS_DIM, dtype=np.float32)\n",
    "            x.append(np.concatenate([\n",
    "                    [ length[pid] , radius[pid] , has_rec ],\n",
    "                    v]))\n",
    "        x = torch.tensor(np.vstack(x), dtype=torch.float32)\n",
    "        # ---------- read labels ----------------------------------------------\n",
    "        with open(os.path.join(run_path,'targetOutput.txt')) as f:\n",
    "            pid_e, r_e , z_e = f.readline().split()[:3]\n",
    "        emitter_row   = id2row[int(pid_e)]\n",
    "        y_pipe        = torch.tensor([emitter_row], dtype=torch.long)\n",
    "        y_coord       = torch.tensor([[float(z_e)/length[int(pid_e)],\n",
    "                                       float(r_e)/radius[int(pid_e)]]],\n",
    "                                     dtype=torch.float32)\n",
    "        return Data(x=x,\n",
    "                    edge_index=edge_index,\n",
    "                    y_pipe=y_pipe,          # scalar\n",
    "                    y_coord=y_coord,        # 1×2\n",
    "                    num_nodes=N)\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a6884a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.  GNN model\n",
    "class EmitterGNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=128, layers=3):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_dim, hidden))\n",
    "        for _ in range(layers - 1):\n",
    "            self.convs.append(SAGEConv(hidden, hidden))\n",
    "        self.cls_head = nn.Linear(hidden, 1)   # node logit\n",
    "        self.reg_head = nn.Linear(hidden, 2)   # (z_rel, r_rel)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, ei = data.x, data.edge_index\n",
    "        for conv in self.convs:\n",
    "            x = F.relu(conv(x, ei))\n",
    "        logits = self.cls_head(x).squeeze(-1)       # (ΣN)\n",
    "        coords = self.reg_head(x)                   # (ΣN,2)\n",
    "        return logits, coords, x                   # return x for possible pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "44d414a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.  Loss\n",
    "def emitter_loss(batch, logits, coords, λ=10.0):\n",
    "    \"\"\"\n",
    "    Graph-aware loss:\n",
    "      • cross-entropy over nodes *within each graph*  (no class-imbalance issues)\n",
    "      • MSE on (z/L , r/R) for the emitter node\n",
    "    \"\"\"\n",
    "    # 1) node-wise probability restricted to each graph\n",
    "    probs = softmax(logits, batch.batch)                 # (ΣN,)\n",
    "\n",
    "    # negative log-likelihood on true emitter indices\n",
    "    emitter_idx = batch.ptr[:-1] + batch.y_pipe.squeeze()\n",
    "    loss_cls = (-torch.log(probs[emitter_idx] + 1e-12)).mean()\n",
    "\n",
    "    # 2) coordinate regression on the emitter rows\n",
    "    pred_coord = coords[emitter_idx]                     # (B,2)\n",
    "    loss_reg   = F.mse_loss(pred_coord, batch.y_coord.squeeze())\n",
    "\n",
    "    return loss_cls + λ * loss_reg, loss_cls.item(), loss_reg.item()\n",
    "# ---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9ed8ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.  Training loop ----------------------------------------------------------\n",
    "def train(root, epochs=200, lr=2e-3, batch_size=24):\n",
    "    ds = VeinTreeDataset(root)\n",
    "    n = len(ds)\n",
    "    train_len = int(0.7*n)\n",
    "    val_len   = int(0.15*n)\n",
    "    test_len  = n - train_len - val_len\n",
    "    train_ds, val_ds, test_ds = random_split(ds,[train_len,val_len,test_len],\n",
    "                                             generator=torch.Generator().manual_seed(42))\n",
    "    loader   = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "    val_lo   = DataLoader(val_ds, batch_size)\n",
    "    test_lo  = DataLoader(test_ds, batch_size)\n",
    "    model = EmitterGNN(in_dim=3+R_STATS_DIM).to(DEVICE)\n",
    "    opt   = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    best = math.inf\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train(); tl=0\n",
    "        for batch in loader:\n",
    "            batch = batch.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            logits, coords,_ = model(batch)\n",
    "            loss,_,_ = emitter_loss(batch, logits, coords)\n",
    "            loss.backward(); opt.step()\n",
    "            tl += loss.item()*batch.num_graphs\n",
    "        # ---- val ------------------------------------------------------------\n",
    "        model.eval(); vl=0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_lo:\n",
    "                batch=batch.to(DEVICE)\n",
    "                logits,coords,_ = model(batch)\n",
    "                loss,_,_ = emitter_loss(batch,logits,coords)\n",
    "                vl += loss.item()*batch.num_graphs\n",
    "        vl /= len(val_ds); tl/=len(train_ds)\n",
    "        if vl<best:\n",
    "            best=vl; torch.save(model.state_dict(),'best_gnn.pt')\n",
    "        if epoch%1==0:\n",
    "            print(f'E{epoch:03d}  train {tl:.4f}  val {vl:.4f}')\n",
    "    # ------------- test ------------------------------------------------------\n",
    "    model.load_state_dict(torch.load('best_gnn.pt')); model.eval()\n",
    "    correct=0; err=[]\n",
    "    with torch.no_grad():\n",
    "        for batch in test_lo:\n",
    "            batch=batch.to(DEVICE)\n",
    "            logits,coords,_=model(batch)\n",
    "            idx = (logits.sigmoid()>0.5).int()        # one emitter per graph\n",
    "            pred = []\n",
    "            for i in range(batch.num_graphs):\n",
    "                beg, end = batch.ptr[i], batch.ptr[i+1]\n",
    "                pred.append(torch.argmax(logits[beg:end]).item())\n",
    "            pred = torch.tensor(pred, device=DEVICE)\n",
    "            correct += (pred==batch.y_pipe.squeeze()).sum().item()\n",
    "            emitter_idx = batch.ptr[:-1]+batch.y_pipe.squeeze()\n",
    "            err.extend(torch.sqrt(((coords[emitter_idx]-batch.y_coord.squeeze())**2).sum(1)).cpu().numpy())\n",
    "    print(f'\\nTest  pipe accuracy {correct/len(test_ds)*100:5.1f}%   '\n",
    "          f'avg 3-D error {np.mean(err):.4f}')\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c00ac709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E001  train 4.4252  val 4.3470\n",
      "E002  train 4.3241  val 4.3500\n",
      "E003  train 4.3147  val 4.2805\n",
      "E004  train 4.3106  val 4.3258\n",
      "E005  train 4.3080  val 4.2711\n",
      "E006  train 4.2968  val 4.3352\n",
      "E007  train 4.3139  val 4.3041\n",
      "E008  train 4.3001  val 4.2908\n",
      "E009  train 4.3031  val 4.3232\n",
      "E010  train 4.3006  val 4.3206\n",
      "E011  train 4.3010  val 4.2707\n",
      "E012  train 4.3088  val 4.3664\n",
      "E013  train 4.3199  val 4.3118\n",
      "E014  train 4.3026  val 4.2873\n",
      "E015  train 4.2968  val 4.2924\n",
      "E016  train 4.2934  val 4.2907\n",
      "E017  train 4.2981  val 4.3291\n",
      "E018  train 4.2932  val 4.2868\n",
      "E019  train 4.3048  val 4.3276\n",
      "E020  train 4.3029  val 4.2993\n",
      "E021  train 4.2987  val 4.2827\n",
      "E022  train 4.3224  val 4.3407\n",
      "E023  train 4.3000  val 4.3124\n",
      "E024  train 4.3160  val 4.2875\n",
      "E025  train 4.3028  val 4.2981\n",
      "E026  train 4.2911  val 4.3232\n",
      "E027  train 4.3004  val 4.3080\n",
      "E028  train 4.2977  val 4.2932\n",
      "E029  train 4.2971  val 4.3323\n",
      "E030  train 4.3080  val 4.2800\n",
      "E031  train 4.2967  val 4.2861\n",
      "E032  train 4.2919  val 4.2914\n",
      "E033  train 4.2951  val 4.2976\n",
      "E034  train 4.2913  val 4.2858\n",
      "E035  train 4.2949  val 4.2843\n",
      "E036  train 4.2928  val 4.2895\n",
      "E037  train 4.2942  val 4.2921\n",
      "E038  train 4.3014  val 4.2840\n",
      "E039  train 4.2943  val 4.3011\n",
      "E040  train 4.2972  val 4.2920\n",
      "E041  train 4.2956  val 4.3104\n",
      "E042  train 4.2979  val 4.2938\n",
      "E043  train 4.2918  val 4.2871\n",
      "E044  train 4.2924  val 4.2899\n",
      "E045  train 4.2924  val 4.2919\n",
      "E046  train 4.2907  val 4.2980\n",
      "E047  train 4.2929  val 4.2924\n",
      "E048  train 4.2914  val 4.2879\n",
      "E049  train 4.2993  val 4.2844\n",
      "E050  train 4.2893  val 4.2884\n",
      "E051  train 4.2899  val 4.2864\n",
      "E052  train 4.2917  val 4.2815\n",
      "E053  train 4.2945  val 4.2950\n",
      "E054  train 4.2940  val 4.2950\n",
      "E055  train 4.2895  val 4.2851\n",
      "E056  train 4.2905  val 4.2781\n",
      "E057  train 4.2909  val 4.2842\n",
      "E058  train 4.2905  val 4.2966\n",
      "E059  train 4.2909  val 4.2919\n",
      "E060  train 4.2929  val 4.2826\n",
      "E061  train 4.2913  val 4.2854\n",
      "E062  train 4.2927  val 4.2891\n",
      "E063  train 4.2936  val 4.2841\n",
      "E064  train 4.2916  val 4.2880\n",
      "E065  train 4.2924  val 4.2889\n",
      "E066  train 4.2912  val 4.2848\n",
      "E067  train 4.2920  val 4.2908\n",
      "E068  train 4.2924  val 4.2869\n",
      "E069  train 4.2986  val 4.2835\n",
      "E070  train 4.2911  val 4.2916\n",
      "E071  train 4.2908  val 4.2838\n",
      "E072  train 4.2923  val 4.2853\n",
      "E073  train 4.2931  val 4.2810\n",
      "E074  train 4.2893  val 4.2930\n",
      "E075  train 4.2948  val 4.2899\n",
      "E076  train 4.2898  val 4.2791\n",
      "E077  train 4.2911  val 4.2940\n",
      "E078  train 4.2907  val 4.2809\n",
      "E079  train 4.2970  val 4.2844\n",
      "E080  train 4.2958  val 4.2801\n",
      "E081  train 4.2942  val 4.2787\n",
      "E082  train 4.2907  val 4.2858\n",
      "E083  train 4.2931  val 4.2950\n",
      "E084  train 4.2965  val 4.2886\n",
      "E085  train 4.2909  val 4.2799\n",
      "E086  train 4.2909  val 4.2878\n",
      "E087  train 4.2908  val 4.2827\n",
      "E088  train 4.2935  val 4.2877\n",
      "E089  train 4.2985  val 4.2899\n",
      "E090  train 4.2999  val 4.2865\n",
      "E091  train 4.2891  val 4.2948\n",
      "E092  train 4.2935  val 4.2915\n",
      "E093  train 4.2888  val 4.2810\n",
      "E094  train 4.2914  val 4.2850\n",
      "E095  train 4.2912  val 4.2823\n",
      "E096  train 4.2892  val 4.2870\n",
      "E097  train 4.2930  val 4.2816\n",
      "E098  train 4.2898  val 4.2863\n",
      "E099  train 4.2900  val 4.2962\n",
      "E100  train 4.2933  val 4.2833\n",
      "E101  train 4.2896  val 4.2943\n",
      "E102  train 4.2908  val 4.2914\n",
      "E103  train 4.2933  val 4.2931\n",
      "E104  train 4.2895  val 4.2830\n",
      "E105  train 4.2999  val 4.2793\n",
      "E106  train 4.3003  val 4.2850\n",
      "E107  train 4.2947  val 4.2830\n",
      "E108  train 4.2929  val 4.2822\n",
      "E109  train 4.2958  val 4.2871\n",
      "E110  train 4.2933  val 4.2909\n",
      "E111  train 4.2935  val 4.2953\n",
      "E112  train 4.3005  val 4.2880\n",
      "E113  train 4.2910  val 4.2960\n",
      "E114  train 4.2911  val 4.2811\n",
      "E115  train 4.2920  val 4.2889\n",
      "E116  train 4.2950  val 4.2905\n",
      "E117  train 4.2910  val 4.2865\n",
      "E118  train 4.2900  val 4.2874\n",
      "E119  train 4.2923  val 4.2868\n",
      "E120  train 4.2906  val 4.2892\n",
      "E121  train 4.2888  val 4.2770\n",
      "E122  train 4.2900  val 4.2980\n",
      "E123  train 4.2927  val 4.2845\n",
      "E124  train 4.2889  val 4.2907\n",
      "E125  train 4.2892  val 4.2856\n",
      "E126  train 4.2903  val 4.2792\n",
      "E127  train 4.2959  val 4.2792\n",
      "E128  train 4.2932  val 4.2835\n",
      "E129  train 4.2905  val 4.3050\n",
      "E130  train 4.2924  val 4.2765\n",
      "E131  train 4.2911  val 4.2847\n",
      "E132  train 4.2887  val 4.2811\n",
      "E133  train 4.2893  val 4.2948\n",
      "E134  train 4.2954  val 4.2928\n",
      "E135  train 4.2909  val 4.2859\n",
      "E136  train 4.2908  val 4.2809\n",
      "E137  train 4.2892  val 4.2760\n",
      "E138  train 4.2909  val 4.2834\n",
      "E139  train 4.2934  val 4.2803\n",
      "E140  train 4.2901  val 4.2928\n",
      "E141  train 4.2938  val 4.2868\n",
      "E142  train 4.2909  val 4.2801\n",
      "E143  train 4.2890  val 4.2899\n",
      "E144  train 4.2938  val 4.3004\n",
      "E145  train 4.2925  val 4.2904\n",
      "E146  train 4.2938  val 4.2899\n",
      "E147  train 4.2898  val 4.2815\n",
      "E148  train 4.2919  val 4.2842\n",
      "E149  train 4.2900  val 4.2874\n",
      "E150  train 4.2925  val 4.2849\n",
      "E151  train 4.2987  val 4.2881\n",
      "E152  train 4.2904  val 4.2835\n",
      "E153  train 4.2903  val 4.2891\n",
      "E154  train 4.2897  val 4.2810\n",
      "E155  train 4.2898  val 4.2941\n",
      "E156  train 4.2904  val 4.2835\n",
      "E157  train 4.2902  val 4.2952\n",
      "E158  train 4.2910  val 4.2866\n",
      "E159  train 4.2914  val 4.2892\n",
      "E160  train 4.2910  val 4.2832\n",
      "E161  train 4.2923  val 4.2983\n",
      "E162  train 4.2923  val 4.2885\n",
      "E163  train 4.2899  val 4.2888\n",
      "E164  train 4.2897  val 4.2826\n",
      "E165  train 4.2922  val 4.2782\n",
      "E166  train 4.2895  val 4.2893\n",
      "E167  train 4.2909  val 4.2879\n",
      "E168  train 4.2895  val 4.2812\n",
      "E169  train 4.2905  val 4.2922\n",
      "E170  train 4.2905  val 4.2939\n",
      "E171  train 4.2909  val 4.2930\n",
      "E172  train 4.2910  val 4.2915\n",
      "E173  train 4.2959  val 4.2900\n",
      "E174  train 4.2895  val 4.2859\n",
      "E175  train 4.2889  val 4.2873\n",
      "E176  train 4.2901  val 4.2893\n",
      "E177  train 4.2898  val 4.2793\n",
      "E178  train 4.2910  val 4.2876\n",
      "E179  train 4.2939  val 4.2984\n",
      "E180  train 4.2925  val 4.2854\n",
      "E181  train 4.2898  val 4.2959\n",
      "E182  train 4.2905  val 4.2878\n",
      "E183  train 4.2895  val 4.2807\n",
      "E184  train 4.2892  val 4.2797\n",
      "E185  train 4.2887  val 4.2870\n",
      "E186  train 4.2935  val 4.2827\n",
      "E187  train 4.2895  val 4.2818\n",
      "E188  train 4.2900  val 4.2865\n",
      "E189  train 4.2925  val 4.2885\n",
      "E190  train 4.2905  val 4.2836\n",
      "E191  train 4.2897  val 4.2795\n",
      "E192  train 4.2886  val 4.2856\n",
      "E193  train 4.2918  val 4.2945\n",
      "E194  train 4.2891  val 4.2828\n",
      "E195  train 4.2894  val 4.2926\n",
      "E196  train 4.2893  val 4.2903\n",
      "E197  train 4.2925  val 4.2912\n",
      "E198  train 4.2911  val 4.2892\n",
      "E199  train 4.2901  val 4.3084\n",
      "E200  train 4.2910  val 4.2820\n",
      "\n",
      "Test  pipe accuracy   6.0%   avg 3-D error 0.5188\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    train('/Users/daghanerdonmez/Desktop/molecular-simulation-mlp/output-processing/Outputs_Copy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7d2bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
