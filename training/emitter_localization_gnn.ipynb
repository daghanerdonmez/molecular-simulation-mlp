{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch_geometric.data import Data, Dataset, DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "MAX_PIPES: int = 40  # accommodates pipe1-pipe39 (index 1-39). we also keep slot 0.\n",
    "NODE_FEAT_DIM: int = 3  # length, radius, num_recv\n",
    "CLS_WEIGHT: float = 5.0  # weight for pipe-classification loss\n",
    "BATCH_SIZE: int = 32\n",
    "EPOCHS: int = 50\n",
    "LR: float = 3e-4\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "#   UTILITIES\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def read_pipe_simulation(path: str) -> Tuple[int, int, float, float, float]:\n",
    "    \"\"\"Read simulation_data.txt and return tuple:\n",
    "    (pipe_id, parent_id, length, radius, num_receivers)\n",
    "    \"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        parts = f.readline().strip().split()\n",
    "    pipe_id = int(parts[0])\n",
    "    parent_id = int(parts[1])\n",
    "    length = float(parts[2])\n",
    "    radius = float(parts[3])\n",
    "    num_recv = float(parts[4]) if len(parts) > 4 else 0.0\n",
    "    return pipe_id, parent_id, length, radius, num_recv\n",
    "\n",
    "\n",
    "def build_graph_from_run(run_dir: str) -> Tuple[Data, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Create a PyG Data graph for one simulation run.\n",
    "\n",
    "    Returns (data_graph, cls_target, reg_target)\n",
    "    cls_target: scalar tensor (0-based emitter pipe index)\n",
    "    reg_target: tensor([r, z])\n",
    "    \"\"\"\n",
    "    # -------- gather node features & edges ----------\n",
    "    node_feats: List[List[float]] = []  # indexed by pipe_id-1 (0-based)\n",
    "    edge_index = []  # list of (src, dst)\n",
    "\n",
    "    # Preload list of pipe folders present.\n",
    "    pipe_folders = [d for d in os.listdir(run_dir) if d.startswith(\"pipe\")]\n",
    "    for pf in pipe_folders:\n",
    "        pipe_id = int(pf.replace(\"pipe\", \"\"))  # 1-based\n",
    "        while len(node_feats) < pipe_id:\n",
    "            node_feats.append([0.0] * NODE_FEAT_DIM)  # pad missing ids with zero rows\n",
    "\n",
    "        sim_path = os.path.join(run_dir, pf, \"simulation_data.txt\")\n",
    "        if not os.path.exists(sim_path):\n",
    "            continue\n",
    "        p_id, parent_id, length, radius, num_recv = read_pipe_simulation(sim_path)\n",
    "        node_feats[p_id - 1] = [length, radius, num_recv]\n",
    "        if parent_id >= 1:\n",
    "            # convert to 0-based indices\n",
    "            edge_index.append([parent_id - 1, p_id - 1])\n",
    "            edge_index.append([p_id - 1, parent_id - 1])  # undirected\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    if not node_feats:  # no pipes? skip\n",
    "        raise ValueError(f\"No pipe data in {run_dir}\")\n",
    "    x = torch.tensor(node_feats, dtype=torch.float32)\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    # -------- target --------\n",
    "    tgt_path = os.path.join(run_dir, \"targetOutput.txt\")\n",
    "    with open(tgt_path, \"r\") as f:\n",
    "        tparts = f.readline().strip().split()\n",
    "    tgt_pipe = int(tparts[0])  # 1-based\n",
    "    tgt_r = float(tparts[1])\n",
    "    tgt_z = float(tparts[2]) if len(tparts) > 2 else 0.0\n",
    "\n",
    "    cls_target = torch.tensor(tgt_pipe - 1, dtype=torch.long)  # 0-based class\n",
    "    reg_target = torch.tensor([tgt_r, tgt_z], dtype=torch.float32)\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, y=cls_target, pos_target=reg_target)\n",
    "    return data, cls_target, reg_target\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "#   DATASET CLASS\n",
    "# ---------------------------------------------------------\n",
    "class PipeGraphDataset(Dataset):\n",
    "    \"\"\"PyG Dataset that loads all simulation runs under a directory.\"\"\"\n",
    "\n",
    "    def __init__(self, root: str):\n",
    "        super().__init__(None, None, None)\n",
    "        self.root_dir = root\n",
    "        self.run_dirs = [os.path.join(root, d) for d in os.listdir(root)\n",
    "                         if os.path.isdir(os.path.join(root, d)) and not d.startswith('.')]\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.run_dirs)\n",
    "\n",
    "    def get(self, idx):\n",
    "        run_dir = self.run_dirs[idx]\n",
    "        data, cls_target, reg_target = build_graph_from_run(run_dir)\n",
    "        return data\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "#   MODEL\n",
    "# ---------------------------------------------------------\n",
    "class PipeGNN(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden: int = 64, num_layers: int = 3, num_classes: int = 39):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(GCNConv(in_dim, hidden))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(GCNConv(hidden, hidden))\n",
    "        self.cls_head = nn.Linear(hidden, num_classes)\n",
    "        self.reg_head = nn.Linear(hidden, 2)\n",
    "\n",
    "    def forward(self, data: Data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index).relu()\n",
    "        # global pooling\n",
    "        x = global_mean_pool(x, batch)\n",
    "        logits = self.cls_head(x)\n",
    "        coords = self.reg_head(x)\n",
    "        return logits, coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "#   TRAIN / VAL / TEST\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def train_epoch(model, loader, opt, cls_crit, reg_crit):\n",
    "    model.train()\n",
    "    total_cls = total_reg = 0.0\n",
    "    n_samples = 0\n",
    "    for data in loader:\n",
    "        data = data.to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "        logits, coords = model(data)\n",
    "        cls_loss = cls_crit(logits, data.y)\n",
    "        reg_loss = reg_crit(coords, data.pos_target)\n",
    "        loss = CLS_WEIGHT * cls_loss + reg_loss\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        bs = data.num_graphs\n",
    "        total_cls += cls_loss.item() * bs\n",
    "        total_reg += reg_loss.item() * bs\n",
    "        n_samples += bs\n",
    "    return total_cls / n_samples, total_reg / n_samples\n",
    "\n",
    "\n",
    "def eval_epoch(model, loader, cls_crit, reg_crit):\n",
    "    model.eval()\n",
    "    total_cls = total_reg = 0.0\n",
    "    correct = 0\n",
    "    pos_errors = []\n",
    "    n_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(DEVICE)\n",
    "            logits, coords = model(data)\n",
    "            cls_loss = cls_crit(logits, data.y)\n",
    "            reg_loss = reg_crit(coords, data.pos_target)\n",
    "\n",
    "            bs = data.num_graphs\n",
    "            total_cls += cls_loss.item() * bs\n",
    "            total_reg += reg_loss.item() * bs\n",
    "            n_samples += bs\n",
    "\n",
    "            pred_pipe = logits.argmax(dim=1)\n",
    "            correct += (pred_pipe == data.y).sum().item()\n",
    "            pos_errors.extend(((coords - data.pos_target).norm(dim=1)).cpu().numpy().tolist())\n",
    "    acc = 100.0 * correct / n_samples\n",
    "    return total_cls / n_samples, total_reg / n_samples, acc, np.mean(pos_errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "#   MAIN\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    data_root = os.path.join(os.path.dirname(__file__), \"../output-processing/Outputs_Copy\")\n",
    "    dataset = PipeGraphDataset(data_root)\n",
    "    print(f\"Total samples: {len(dataset)}\")\n",
    "\n",
    "    # Split dataset\n",
    "    idxs = np.random.permutation(len(dataset))\n",
    "    train_split = int(0.7 * len(dataset))\n",
    "    val_split = int(0.85 * len(dataset))\n",
    "    train_idx, val_idx, test_idx = idxs[:train_split], idxs[train_split:val_split], idxs[val_split:]\n",
    "    train_ds = dataset.index_select(train_idx.tolist())\n",
    "    val_ds = dataset.index_select(val_idx.tolist())\n",
    "    test_ds = dataset.index_select(test_idx.tolist())\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "    model = PipeGNN(in_dim=NODE_FEAT_DIM, hidden=128, num_layers=3, num_classes=39).to(DEVICE)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    cls_crit = nn.CrossEntropyLoss()\n",
    "    reg_crit = nn.MSELoss()\n",
    "\n",
    "    best_val = float('inf')\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        trc, trr = train_epoch(model, train_loader, opt, cls_crit, reg_crit)\n",
    "        vcc, vcr, vacc, verr = eval_epoch(model, val_loader, cls_crit, reg_crit)\n",
    "        print(f\"Epoch {epoch:02d} | \"\n",
    "              f\"train cls {trc:.4f} reg {trr:.4f} | \"\n",
    "              f\"val cls {vcc:.4f} reg {vcr:.4f} acc {vacc:.2f}% posErr {verr:.4f}\")\n",
    "        if vcc + vcr < best_val:\n",
    "            best_val = vcc + vcr\n",
    "            torch.save(model.state_dict(), \"best_pipe_gnn.pt\")\n",
    "\n",
    "    # ---- test ----\n",
    "    tcc, tcr, tacc, terr = eval_epoch(model, test_loader, cls_crit, reg_crit)\n",
    "    print(\"Test results | \"\n",
    "          f\"cls loss {tcc:.4f} reg loss {tcr:.4f} | \"\n",
    "          f\"acc {tacc:.2f}% posErr {terr:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        import torch_geometric  # noqa\n",
    "    except ImportError:\n",
    "        raise ImportError(\"torch_geometric is required. Install via 'pip install torch-geometric torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-$(torch.version)[...]')\")\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
